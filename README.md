# chat_JoPT_0.1
A Large Language Model (LLM) designed to mimick the behavior of Poligang's stupidest member using the latest state of the art (SOTA) techniques in behavioral science and generative NLP. 

In order to replicate Jonathan's behaviors, techniques such as learnable latent embeddings for neural analysis (https://arxiv.org/abs/2204.00673v2) were used. A model drift detection tool
made for LLMs was also implemented to make sure that Chat JoPT's behavior remains as close as possible to Jonathan's behavior (a constant state of tiredness, confusion and unexpected 
enthousiasm for the most random things in life, i.e. the behavior of a newborn that is 5 weeks of age.)

Chat JoPT stands for Jonathan Pretrained Transformer, which basically means nothing, as expected of the creator of this model. This model is available in multiple versions, including a 
0 billion weight model (yields closest answers to what Jonathan would say) and a 10 billion weight model.

![image](https://github.com/ZavalasBarber/chat_JoPT_0.1/assets/64659920/cd4ab570-b167-43ed-9593-179114abfcdb)
